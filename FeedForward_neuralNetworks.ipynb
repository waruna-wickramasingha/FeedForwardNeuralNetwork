{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from numpy import loadtxt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nimport pandas as pd \nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/creditscreening/credit-screening.data')\n\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colNames = []\n\nfor i in range(15):\n    x = \"A\" + str(i+1)\n    colNames.append(x)\n\ncolNames.append('class')\ndataset.columns = colNames\ndataset.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.replace('?', np.nan, inplace=True)\ndataset.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.fillna(method ='pad')\ndataset.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['A14'] = dataset['A14'].astype('int64')\ndataset['A2'] = dataset['A2'].astype('float64')\ndataset.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['A1'] = dataset['A1'].astype('category')\ndataset['A4'] = dataset['A4'].astype('category')\ndataset['A5'] = dataset['A5'].astype('category')\ndataset['A6'] = dataset['A6'].astype('category')\ndataset['A7'] = dataset['A7'].astype('category')\ndataset['A9'] = dataset['A9'].astype('category')\ndataset['A10'] = dataset['A10'].astype('category')\ndataset['A12'] = dataset['A12'].astype('category')\ndataset['A13'] = dataset['A13'].astype('category')\nprint(dataset.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['A1'] = dataset['A1'].cat.codes\ndataset['A4'] = dataset['A4'].cat.codes\ndataset['A5'] = dataset['A5'].cat.codes\ndataset['A6'] = dataset['A6'].cat.codes\ndataset['A7'] = dataset['A7'].cat.codes\ndataset['A9'] = dataset['A9'].cat.codes\ndataset['A10'] = dataset['A10'].cat.codes\ndataset['A12'] = dataset['A12'].cat.codes\ndataset['A13'] = dataset['A13'].cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### setting numpy seed for repeatability."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(1337)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.to_csv('credit-screening-all-numerics.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=5,random_state=42, shuffle=False)\n\nX = dataset.iloc[:,:-1]\nY = dataset.iloc[:,-1]\n\nY.replace('+', 1, inplace=True)\nY.replace('-', 0, inplace=True)\n\nprint(len(X.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalizing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = (X-X.min())/(X.max()-X.min())\nprint(X.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def doTrainAndEvaluation(hiddenlayerNodeCount, hiddenLayerActivation, outputLayerActivation, lossFunction, plot_history=False):\n    print(\"DoTrainAndEvaluation with hidden nodes=%s\" %hiddenlayerNodeCount)\n    \n    scores = []\n    fold = 0        \n    \n    for train_index, test_index in skf.split(X, Y):\n        fold = fold + 1\n        x_train, x_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], Y.iloc[train_index], Y.iloc[test_index]\n        \n        model = Sequential()\n        model.add(Dense(hiddenlayerNodeCount, input_dim=15, activation=hiddenLayerActivation, kernel_initializer='normal'))\n        model.add(Dense(1, activation=outputLayerActivation, kernel_initializer='normal'))\n        model.compile(loss=lossFunction, optimizer='adam', metrics=['acc'])\n        \n        history = model.fit(x_train, y_train, epochs=50, verbose=0, batch_size=100, validation_split=0.2)\n        \n        if plot_history == True:\n            print(\"###### Cross validation fold number = %s\" %fold)\n            plt.plot(history.history['acc'])\n            plt.plot(history.history['loss'])\n            plt.plot(history.history['val_loss'])\n            plt.title('Model loss')\n            plt.ylabel('Loss')\n            plt.xlabel('Epoch')\n            plt.legend(['accuracy','Train', 'Valildation'], loc='upper right')\n            plt.show()\n        \n        y_pred = model.predict(x_test, verbose=0)\n        y_pred = np.where(y_pred > 0.5, 1, 0)\n        f1 = f1_score(y_test, y_pred, average='macro')\n        scores.append(f1)\n\n    print(\"Hidden layer count={} Mean values for f1={}\".format(hiddenlayerNodeCount, np.mean(scores, axis=0)))\n    print(\"========================================Model complete========================================\")\n    \n    return np.mean(scores, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pylab as plt\n\ndef plot_summary(result_dict, s = \"hidden layer neuron count\"):      \n    items = result_dict.items()\n    x,y = zip(*items)\n    plt.plot(x, y)\n    plt.xlabel(s)\n    plt.ylabel('F1 value')\n    maximum_f1_value = max(y)\n    hidden_neuron_count = max(result_dict, key=lambda k: result_dict[k])\n\n    print(\"Maximum f1 val=\" + str(maximum_f1_value) + \", \" + s + \"=\" +  str(hidden_neuron_count))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 1"},{"metadata":{},"cell_type":"markdown","source":"* loss=mean_squared_error\n* activaion=sigmoid\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'sigmoid', 'sigmoid', 'mean_squared_error', plot_history=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 2\n\n* loss=binary_crossentropy\n* activaion=sigmoid\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'sigmoid', 'sigmoid', 'binary_crossentropy', plot_history=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 3\n* loss=binary_crossentropy\n* activaion=rectified liner unit (relu)\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'relu', 'relu', 'binary_crossentropy', plot_history=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 4\n* loss=mean_squared_error\n* activaion=rectified liner unit (relu)\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'relu', 'relu', 'mean_squared_error', plot_history=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 5\n* loss=mean_squared_error\n* activaion=tanh\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'tanh', 'tanh', 'mean_squared_error', plot_history=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 6\n* loss=binary_crossentropy\n* activaion=tanh\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'tanh', 'tanh', 'binary_crossentropy', plot_history=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 7\n* loss=mean_squared_error\n* activaion=linear\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'linear', 'linear', 'mean_squared_error', plot_history=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Model 7.1\n* loss=binary_crossentropy\n* activaion=linear\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'linear', 'linear', 'binary_crossentropy', plot_history=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 8\n* loss=mean_squared_error\n* activaion=sigmoid and tanh\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'sigmoid', 'tanh', 'mean_squared_error', plot_history=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 8.1\n* loss=binary_crossentropy\n\n* activaion=sigmoid and tanh\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'sigmoid', 'tanh', 'binary_crossentropy', plot_history=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 9\n* loss=mean_squared_error\n* activaion= tanh and sigmoid\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'tanh', 'sigmoid', 'mean_squared_error', plot_history=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 9.1\n* loss=binary_crossentropy\n* activaion= tanh and sigmoid\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluation(i, 'tanh', 'sigmoid', 'binary_crossentropy', plot_history=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Two hidden layer configurations"},{"metadata":{"trusted":true},"cell_type":"code","source":"def doTrainAndEvaluationTwoHiddenLayers(hidL1NodeCount, hidL1Activation,hidL2NodeCount, hidL2Activation, outputLActivation, lossFunction, plot_history=False):\n    print(\"DoTrainAndEvaluation with hidden Layer 1 nodes={} Hidden layer2 nodes={}\".format(hidL1NodeCount,hidL2NodeCount))\n    \n    scores = []\n    fold = 0        \n    \n    for train_index, test_index in skf.split(X, Y):\n        fold = fold+1\n        x_train, x_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], Y.iloc[train_index], Y.iloc[test_index]\n        \n        model = Sequential()\n        model.add(Dense(hidL1NodeCount, input_dim=15, activation=hidL1Activation, kernel_initializer='normal'))\n        model.add(Dense(hidL2NodeCount, activation=hidL2Activation, kernel_initializer='normal'))\n        model.add(Dense(1, activation=outputLActivation, kernel_initializer='normal'))\n        model.compile(loss=lossFunction, optimizer='adam', metrics=['acc'])\n        \n        history = model.fit(x_train, y_train, epochs=50, verbose=0, batch_size=100, validation_split=0.2)\n        \n        if plot_history == True:\n            print(\"###### Cross validation fold number = %s\" %fold)\n            plt.plot(history.history['acc'])\n            plt.plot(history.history['loss'])\n            plt.plot(history.history['val_loss'])\n            plt.title('History plot on Model loss')\n            plt.ylabel('loss')\n            plt.xlabel('Epoch')\n            plt.legend(['Acc','Train', 'Test'], loc='upper left')\n            plt.show()\n        \n        y_pred = model.predict(x_test, verbose=0)\n        y_pred = np.where(y_pred > 0.5, 1, 0)\n        f1 = f1_score(y_test, y_pred, average='macro')\n        scores.append(f1)\n\n    print(\"Mean values for f1={}\".format(np.mean(scores, axis=0)))\n    print(\"========================================Model complete========================================\")\n    \n    return np.mean(scores, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 10\n* loss=mean_squared_error\n* activaion= sigmoid\n* inputlayer=15\n* outputlayer=1\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluationTwoHiddenLayers(i, 'sigmoid', i, 'sigmoid', 'sigmoid', 'mean_squared_error', plot_history=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 11\n* loss=mean_squared_error\n* activaion= tanh\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluationTwoHiddenLayers(i, 'tanh', i, 'tanh', 'tanh', 'mean_squared_error', plot_history=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 12\n* loss=binary_crossentropy\n* activaion= tanh\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluationTwoHiddenLayers(i, 'tanh', i, 'tanh', 'tanh', 'binary_crossentropy', plot_history=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 13\n* loss=binary_crossentropy\n* activaion= tanh and sigmoid\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 16, 1):\n    matrices_variation[i] = doTrainAndEvaluationTwoHiddenLayers(i, 'sigmoid', i, 'tanh', 'tanh', 'binary_crossentropy', plot_history=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 7 above has given out the best F1 value\n\n## Introduce Normalzation to above model 7"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import regularizers\nimport numpy\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict \n\ndef addRegularization(regularization, showHistory=False):\n    variation = defaultdict(list)\n    scores = []\n    fold = 0        \n    for train_index, test_index in skf.split(X, Y):\n        fold = fold+1\n        x_train, x_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], Y.iloc[train_index], Y.iloc[test_index]\n\n        for i in numpy.arange(0, 0.01, 0.001): \n            model = Sequential()\n            if (regularization == \"l1\"):\n                model.add(Dense(14, input_dim=15, activation='linear', kernel_regularizer=regularizers.l1(i)))\n                model.add(Dense(1, activation='linear', kernel_regularizer=regularizers.l1(i)))\n            else:\n                model.add(Dense(14, input_dim=15, activation='linear', kernel_regularizer=regularizers.l2(i)))\n                model.add(Dense(1, activation='linear', kernel_regularizer=regularizers.l2(i)))\n                \n            model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n            history = model.fit(x_train, y_train, epochs=50, verbose=0, validation_split=0.2)\n\n            if showHistory == True:\n                print(\"###### Cross validation fold number = %s\" %fold)\n                plt.plot(history.history['acc'])\n                plt.plot(history.history['val_loss'])\n                plt.plot(history.history['loss'])\n                plt.title('Model loss and accuracy')\n                plt.ylabel('loss and accuracy')\n                plt.xlabel('Epoch')\n                plt.legend(['accuracy', 'validation loss', 'loss'], loc='upper right')\n                plt.show()\n\n            y_pred = model.predict(x_test, verbose=0)\n            y_pred = np.where(y_pred > 0.5, 1, 0)\n            f1 = f1_score(y_test, y_pred, average='macro')\n            scores.append(f1)\n           \n            variation[i].append(f1)\n\n    return variation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding L1 normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = addRegularization('l1', False)\nprint(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_f1_perfl1 = dict()\n\nfor key, val in matrices_variation.items():\n    avg_f1_perfl1[key] = np.mean(val)\n    \nprint(avg_f1_perfl1)\nplot_summary(avg_f1_perfl1,\"F1 value variation with different lambda in L1\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Introduction of L1 regularization has improved the model by giving a f1 value of 0.8488710330237247 when lambda=0.009"},{"metadata":{},"cell_type":"markdown","source":"### Adding L2 normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = addRegularization('l2')\nprint(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_f1_perfl2 = dict()\n\nfor key, val in matrices_variation.items():\n    avg_f1_perfl2[key] = np.mean(val)\n    \nprint(avg_f1_perfl2)\nplot_summary(avg_f1_perfl2, \"F1 value variation with different lambda in L2\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As it can be seen above, introduction of L2 rgularization has Not improved the model"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}