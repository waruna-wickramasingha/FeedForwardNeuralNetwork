{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from numpy import loadtxt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nimport pandas as pd \nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/creditscreening/credit-screening.data')\n\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colNames = []\n\nfor i in range(15):\n    x = \"A\" + str(i+1)\n    colNames.append(x)\n\ncolNames.append('class')\ndataset.columns = colNames\ndataset.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.replace('?', np.nan, inplace=True)\ndataset.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.fillna(method ='pad')\ndataset.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['A14'] = dataset['A14'].astype('int64')\ndataset['A2'] = dataset['A2'].astype('float64')\ndataset.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['A1'] = dataset['A1'].astype('category')\ndataset['A4'] = dataset['A4'].astype('category')\ndataset['A5'] = dataset['A5'].astype('category')\ndataset['A6'] = dataset['A6'].astype('category')\ndataset['A7'] = dataset['A7'].astype('category')\ndataset['A9'] = dataset['A9'].astype('category')\ndataset['A10'] = dataset['A10'].astype('category')\ndataset['A12'] = dataset['A12'].astype('category')\ndataset['A13'] = dataset['A13'].astype('category')\nprint(dataset.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['A1'] = dataset['A1'].cat.codes\ndataset['A4'] = dataset['A4'].cat.codes\ndataset['A5'] = dataset['A5'].cat.codes\ndataset['A6'] = dataset['A6'].cat.codes\ndataset['A7'] = dataset['A7'].cat.codes\ndataset['A9'] = dataset['A9'].cat.codes\ndataset['A10'] = dataset['A10'].cat.codes\ndataset['A12'] = dataset['A12'].cat.codes\ndataset['A13'] = dataset['A13'].cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### setting numpy seed for repeatability."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(1337)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.to_csv('credit-screening-all-numerics.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\ncv = KFold(n_splits=5, random_state=42, shuffle=False)\n\n# X = one_hot_encoded_data.iloc[:,:-1]\nX = dataset.iloc[:,:-1]\nY = dataset.iloc[:,-1]\n\nY.replace('+', 1, inplace=True)\nY.replace('-', 0, inplace=True)\n\nprint(len(X.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalizing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = (X-X.min())/(X.max()-X.min())\nprint(X.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def doTrainAndEvaluation(model, hiddenlayerCount):\n    \n    scores = []\n    \n    for train_index, test_index in cv.split(X):\n        x_train, x_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], Y.iloc[train_index], Y.iloc[test_index]\n        model.fit(x_train, y_train, batch_size=20, epochs=10)\n        y_pred = model.predict(x_test, batch_size=100, verbose=1)\n        y_pred = np.where(y_pred > 0.5, 1, 0)\n        f1 = f1_score(y_test, y_pred, average='macro')\n        scores.append(f1)\n\n    print(\"\")\n    print(\"##########################################################################################\")\n\n    print(\"Hidden layer count %s Mean values for f1:\" % hiddenlayerCount)\n    print(np.mean(scores, axis=0))\n    \n    return np.mean(scores, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pylab as plt\n\ndef plot_summary(result_dict, s = \"hidden layer neuron count\"):      \n    items = result_dict.items()\n    x,y = zip(*items)\n    plt.plot(x, y)\n    plt.xlabel(s)\n    plt.ylabel('F1 value')\n    maximum_f1_value = max(y)\n    hidden_neuron_count = max(result_dict, key=lambda k: result_dict[k])\n\n    print(\"Maximum f1 val=\" + str(maximum_f1_value) + \", \" + s + \"=\" +  str(hidden_neuron_count))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 1"},{"metadata":{},"cell_type":"markdown","source":"* loss=mean_squared_error\n* activaion=sigmoid\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 15, 1):\n    model1 = Sequential()\n    model1.add(Dense(i, input_dim=15, activation='sigmoid'))\n    model1.add(Dense(1, activation='sigmoid'))\n    model1.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n\n    matrices_variation[i] = doTrainAndEvaluation(model1, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 2\n\n* loss=binary_crossentropy\n* activaion=sigmoid\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 15, 1):\n    model2 = Sequential()\n    model2.add(Dense(i, input_dim=15, activation='sigmoid'))\n    model2.add(Dense(1, activation='sigmoid'))\n    model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n\n    matrices_variation[i] = doTrainAndEvaluation(model2, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 3\n* loss=binary_crossentropy\n* activaion=rectified liner unit (relu)\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 15, 1):\n    model3 = Sequential()\n    model3.add(Dense(i, input_dim=15, activation='relu'))\n    model3.add(Dense(1, activation='relu'))\n    model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n\n    matrices_variation[i] = doTrainAndEvaluation(model3, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 4\n* loss=mean_squared_error\n* activaion=rectified liner unit (relu)\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 15, 1):\n    model4 = Sequential()\n    model4.add(Dense(i, input_dim=15, activation='relu'))\n    model4.add(Dense(1, activation='relu'))\n    model4.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n\n    matrices_variation[i] = doTrainAndEvaluation(model4, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 5\n* loss=mean_squared_error\n* activaion=tanh\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 15, 1):\n    model5 = Sequential()\n    model5.add(Dense(i, input_dim=15, activation='tanh'))\n    model5.add(Dense(1, activation='tanh'))\n    model5.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n\n    matrices_variation[i] = doTrainAndEvaluation(model5, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 6\n* loss=binary_crossentropy\n* activaion=tanh\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 15, 1):\n    model6 = Sequential()\n    model6.add(Dense(i, input_dim=15, activation='tanh'))\n    model6.add(Dense(1, activation='tanh'))\n    model6.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n\n    matrices_variation[i] = doTrainAndEvaluation(model6, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 7\n* loss=mean_squared_error\n* activaion=linear\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 15, 1):\n    model7 = Sequential()\n    model7.add(Dense(i, input_dim=15, activation='linear'))\n    model7.add(Dense(1, activation='linear'))\n    model7.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n\n    matrices_variation[i] = doTrainAndEvaluation(model7, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Model 7.1\n* loss=binary_crossentropy\n* activaion=linear\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 15, 1):\n    model7_1 = Sequential()\n    model7_1.add(Dense(i, input_dim=15, activation='linear'))\n    model7_1.add(Dense(1, activation='linear'))\n    model7_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n\n    matrices_variation[i] = doTrainAndEvaluation(model7_1, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 8\n* loss=mean_squared_error\n* activaion=sigmoid and tanh\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 15, 1):\n    model8 = Sequential()\n    model8.add(Dense(i, input_dim=15, activation='sigmoid'))\n    model8.add(Dense(1, activation='tanh'))\n    model8.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n\n    matrices_variation[i] = doTrainAndEvaluation(model8, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 8.1\n* loss=binary_crossentropy\n\n* activaion=sigmoid and tanh\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 15, 1):\n    model8_1 = Sequential()\n    model8_1.add(Dense(i, input_dim=15, activation='sigmoid'))\n    model8_1.add(Dense(1, activation='tanh'))\n    model8_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n\n    matrices_variation[i] = doTrainAndEvaluation(model8_1, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 9\n* loss=mean_squared_error\n* activaion= tanh and sigmoid\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 15, 1):\n    model9 = Sequential()\n    model9.add(Dense(i, input_dim=15, activation='tanh'))\n    model9.add(Dense(1, activation='sigmoid'))\n    model9.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n\n    matrices_variation[i] = doTrainAndEvaluation(model9, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 9.1\n* loss=binary_crossentropy\n* activaion= tanh and sigmoid\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 15, 1):\n    model9_1 = Sequential()\n    model9_1.add(Dense(i, input_dim=15, activation='tanh'))\n    model9_1.add(Dense(1, activation='sigmoid'))\n    model9_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n\n    matrices_variation[i] = doTrainAndEvaluation(model9_1, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 10\n* loss=mean_squared_error\n* activaion= sigmoid\n* inputlayer=15\n* outputlayer=1\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 15, 1):\n    model10 = Sequential()\n    model10.add(Dense(i, input_dim=15, activation='sigmoid'))\n    model10.add(Dense(i, activation='sigmoid'))\n    model10.add(Dense(1, activation='sigmoid'))\n    model10.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n\n    matrices_variation[i] = doTrainAndEvaluation(model10, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 11\n* loss=mean_squared_error\n* activaion= tanh\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 15, 1):\n    model11 = Sequential()\n    model11.add(Dense(i, input_dim=15, activation='tanh'))\n    model11.add(Dense(i, activation='tanh'))\n    model11.add(Dense(1, activation='tanh'))\n    model11.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n\n    matrices_variation[i] = doTrainAndEvaluation(model11, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 12\n* loss=binary_crossentropy\n* activaion= tanh\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 15, 1):\n    model12 = Sequential()\n    model12.add(Dense(i, input_dim=15, activation='tanh'))\n    model12.add(Dense(i, activation='tanh'))\n    model12.add(Dense(1, activation='tanh'))\n    model12.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n\n    matrices_variation[i] = doTrainAndEvaluation(model12, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 13\n* loss=binary_crossentropy\n* activaion= tanh and sigmoid\n* inputlayer=15\n* outputlayer=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in range(1, 15, 1):\n    model13 = Sequential()\n    model13.add(Dense(i, input_dim=15, activation='sigmoid'))\n    model13.add(Dense(i, activation='tanh'))\n    model13.add(Dense(1, activation='tanh'))\n    model13.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n\n    matrices_variation[i] = doTrainAndEvaluation(model13, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 9.1 above has given out the best F1 value\n\n## Introduce Normalzation to above model 9.1"},{"metadata":{},"cell_type":"markdown","source":"### Adding L1 normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import regularizers\nimport numpy\n\nmatrices_variation = dict() \n\nfor i in numpy.arange(0, 0.01, 0.001):\n    model = Sequential()\n    model.add(Dense(12, input_dim=15, activation='tanh', kernel_regularizer=regularizers.l1(i)))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n    \n    matrices_variation[i] = doTrainAndEvaluation(model, 12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(matrices_variation)\nplot_summary(matrices_variation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding L2 normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrices_variation = dict()\n\nfor i in numpy.arange(0, 0.01, 0.001):\n    model = Sequential()\n    model.add(Dense(12, input_dim=15, activation='tanh', kernel_regularizer=regularizers.l2(i)))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n    \n    matrices_variation[i] = doTrainAndEvaluation(model, 12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(matrices_variation)\nplot_summary(matrices_variation, \"Regularization penalty\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}